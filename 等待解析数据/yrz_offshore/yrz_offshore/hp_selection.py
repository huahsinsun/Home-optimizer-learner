import torch
import numpy as np
from botorch.utils.transforms import standardize
from botorch.utils.multi_objective.box_decompositions import DominatedPartitioning

SMOKE_TEST = 0


def hp_calculation(y_pr, y_apr, y_cont, ref_rate):
    y = torch.cat((y_pr.view(-1, 2), y_apr.view(-1, 2), y_cont.view(-1, 2)), dim=0)

    # mean_y = torch.mean(y, dim=0)
    # std_y = torch.std(y, dim=0)

    # def normalize_tensor(tensor, mean, std):
    #     # Normalize each group of 3 columns in the tensor
    #     for i in range(0, tensor.shape[1], 2):
    #         tensor[:, i:i + 2] = (tensor[:, i:i + 2] - mean) / std
    #     return tensor

    # Normalize each tensor
    # normalized_y_pr = normalize_tensor(y_pr, mean_y, std_y)
    # normalized_y_apr = normalize_tensor(y_apr, mean_y, std_y)
    # normalized_y_cont = normalize_tensor(y_cont, mean_y, std_y)
    nor_y = standardize(y)
    y_pr = standardize(y_pr)
    y_apr = standardize(y_apr)
    y_cont = standardize(y_cont)

    ref_point = torch.min(nor_y, dim=0).values - ref_rate * torch.max(nor_y, dim=0).values
    if not SMOKE_TEST:
        hv_ambo, hv_par, hv_nehv = np.empty([(y_cont.shape[0] - 20) // 2, y_cont.shape[1] // 2]), np.empty(
            [(y_pr.shape[0] - 20) // 2, y_pr.shape[1] // 2]), np.empty(
            [(y_apr.shape[0] - 20) // 2, y_apr.shape[1] // 2])
    else:
        hv_ambo, hv_par, hv_nehv = np.empty([(y_cont.shape[0]) // 2, y_cont.shape[1] // 2]), np.empty(
            [(y_pr.shape[0]) // 2, y_pr.shape[1] // 2]), np.empty(
            [(y_apr.shape[0]) // 2, y_apr.shape[1] // 2])
    # hv_ambo, hv_par, hv_nehv = np.empty([(y_cont.shape[0]-20)//3,y_cont.shape[1]//3]), np.empty([(y_pr.shape[0]-20)//3,y_pr.shape[1]//3]), np.empty([(y_apr.shape[0]-20)//3,y_apr.shape[1]//3])

    for i in range(hv_par.shape[1]):
        for j in range(hv_par.shape[0]):
            bd = DominatedPartitioning(ref_point=ref_point, Y=y_pr[0:20 + j * 2, i * 2:i * 2 + 2])
            volume = bd.compute_hypervolume().item()
            # Append the computed hypervolume to the list
            hv_par[j, i] = volume
    hv_pr_mean = np.mean(hv_par, axis=1)

    for i in range(hv_nehv.shape[1]):
        for j in range(hv_nehv.shape[0]):
            bd = DominatedPartitioning(ref_point=ref_point, Y=y_apr[0:20 + j * 2, i * 2:i * 2 + 2])
            volume = bd.compute_hypervolume().item()
            # Append the computed hypervolume to the list
            hv_nehv[j, i] = volume
    hv_apr_mean = np.mean(hv_nehv, axis=1)

    for i in range(hv_ambo.shape[1]):
        for j in range(hv_ambo.shape[0]):
            bd = DominatedPartitioning(ref_point=ref_point, Y=y_cont[0:20 + j * 2, i * 2:i * 2 + 2])
            volume = bd.compute_hypervolume().item()
            # Append the computed hypervolume to the list
            hv_ambo[j, i] = volume
    hv_cont_mean = np.mean(hv_ambo, axis=1)

    return hv_pr_mean, hv_apr_mean, hv_cont_mean


a = torch.tensor([[-1.2150e+09, 2.5337e+06],
                  [-1.2379e+09, 1.4909e+06],
                  [-1.2298e+09, 2.2004e+06],
                  [-1.1993e+09, 1.9187e+06],
                  [-1.3148e+09, 1.9170e+06],
                  [-1.3547e+09, 1.7259e+06],
                  [-1.2916e+09, 1.4909e+06],
                  [-1.1373e+09, 2.4322e+06],
                  [-1.1707e+09, 2.9404e+06],
                  [-1.2280e+09, 1.4909e+06],
                  [-1.2844e+09, 1.4070e+06],
                  [-1.2310e+09, 2.2032e+06],
                  [-1.2564e+09, 2.3025e+06],
                  [-1.2311e+09, 1.4151e+06],
                  [-1.3042e+09, 1.4909e+06],
                  [-1.3076e+09, 1.9130e+06],
                  [-1.3947e+09, 1.3862e+06],
                  [-1.4033e+09, 1.4028e+06],
                  [-1.4767e+09, 1.4909e+06],
                  [-1.3228e+09, 1.4078e+06],
                  [-1.1717e+09, 2.9378e+06],
                  [-1.1747e+09, 2.9386e+06],
                  [-1.1802e+09, 2.9396e+06],
                  [-1.1762e+09, 2.9399e+06],
                  [-1.1755e+09, 2.9383e+06],
                  [-1.2009e+09, 2.9222e+06],
                  [-1.1697e+09, 2.9449e+06],
                  [-1.1421e+09, 2.4509e+06],
                  [-1.2119e+09, 2.1743e+06],
                  [-1.1288e+09, 2.4957e+06],
                  [-1.1177e+09, 2.5626e+06],
                  [-1.1806e+09, 2.9350e+06],
                  [-1.1473e+09, 2.9509e+06],
                  [-1.1437e+09, 2.9313e+06],
                  [-1.1379e+09, 2.9652e+06],
                  [-1.1358e+09, 2.9556e+06],
                  [-1.1307e+09, 2.9471e+06],
                  [-1.1219e+09, 2.9468e+06],
                  [-1.1318e+09, 2.9517e+06],
                  [-1.1319e+09, 2.9284e+06],
                  [-1.1139e+09, 2.9457e+06],
                  [-1.1286e+09, 2.9037e+06],
                  [-1.1451e+09, 2.7300e+06],
                  [-1.1189e+09, 2.9647e+06],
                  [-1.0873e+09, 2.8387e+06],
                  [-1.0722e+09, 2.9063e+06],
                  [-1.1220e+09, 2.8867e+06],
                  [-1.0166e+09, 2.8886e+06],
                  [-1.0522e+09, 2.6900e+06],
                  [-1.0436e+09, 2.9767e+06],
                  [-1.0435e+09, 2.9955e+06],
                  [-1.0153e+09, 2.8904e+06],
                  [-1.0535e+09, 2.9955e+06],
                  [-1.0539e+09, 2.9955e+06],
                  [-1.1182e+09, 2.9369e+06],
                  [-1.0068e+09, 2.9901e+06],
                  [-1.0591e+09, 2.9955e+06],
                  [-1.0471e+09, 2.9955e+06],
                  [-1.0658e+09, 2.9955e+06],
                  [-1.0716e+09, 2.9955e+06],
                  [-1.0596e+09, 2.9955e+06],
                  [-1.0796e+09, 2.9955e+06],
                  [-1.0722e+09, 2.9638e+06],
                  [-1.0724e+09, 2.9955e+06],
                  [-1.0542e+09, 2.9955e+06],
                  [-9.9825e+08, 2.9814e+06],
                  [-1.0662e+09, 2.9955e+06],
                  [-1.0749e+09, 2.9955e+06],
                  [-1.0464e+09, 2.9502e+06],
                  [-1.0599e+09, 2.9955e+06],
                  [-1.1443e+09, 2.9801e+06],
                  [-1.0675e+09, 2.9955e+06],
                  [-1.0704e+09, 2.9955e+06],
                  [-1.0834e+09, 2.9955e+06]], dtype=torch.float64)
train_obj_pr_analytic = torch.tensor([[-1.2150e+09, 2.5337e+06],
                                      [-1.2379e+09, 1.4909e+06],
                                      [-1.2298e+09, 2.2004e+06],
                                      [-1.1993e+09, 1.9187e+06],
                                      [-1.3148e+09, 1.9170e+06],
                                      [-1.3547e+09, 1.7259e+06],
                                      [-1.2916e+09, 1.4909e+06],
                                      [-1.1373e+09, 2.4322e+06],
                                      [-1.1707e+09, 2.9404e+06],
                                      [-1.2280e+09, 1.4909e+06],
                                      [-1.2844e+09, 1.4070e+06],
                                      [-1.2310e+09, 2.2032e+06],
                                      [-1.2564e+09, 2.3025e+06],
                                      [-1.2311e+09, 1.4151e+06],
                                      [-1.3042e+09, 1.4909e+06],
                                      [-1.3076e+09, 1.9130e+06],
                                      [-1.3947e+09, 1.3862e+06],
                                      [-1.4033e+09, 1.4028e+06],
                                      [-1.4767e+09, 1.4909e+06],
                                      [-1.3228e+09, 1.4078e+06],
                                      [-1.4878e+09, 1.4909e+06],
                                      [-1.3933e+09, 1.3996e+06],
                                      [-1.3120e+09, 1.8634e+06],
                                      [-1.2471e+09, 2.3737e+06],
                                      [-1.2472e+09, 2.5971e+06],
                                      [-1.2148e+09, 2.2438e+06],
                                      [-1.2237e+09, 2.0648e+06],
                                      [-1.2195e+09, 2.2576e+06],
                                      [-1.1129e+09, 2.5933e+06],
                                      [-1.4159e+09, 1.4151e+06],
                                      [-1.2336e+09, 2.0957e+06],
                                      [-1.2300e+09, 2.4547e+06],
                                      [-1.3880e+09, 1.4909e+06],
                                      [-1.4041e+09, 1.4909e+06],
                                      [-1.1844e+09, 2.5503e+06],
                                      [-1.1305e+09, 1.9759e+06],
                                      [-1.4644e+09, 1.7982e+06],
                                      [-1.2449e+09, 2.2807e+06],
                                      [-1.3930e+09, 1.4009e+06],
                                      [-1.1637e+09, 2.3737e+06],
                                      [-1.3311e+09, 1.4081e+06],
                                      [-1.2049e+09, 2.7193e+06],
                                      [-1.3819e+09, 1.3909e+06],
                                      [-1.1525e+09, 2.9614e+06],
                                      [-1.5075e+09, 1.4157e+06],
                                      [-1.2113e+09, 2.5008e+06],
                                      [-1.4479e+09, 1.8098e+06],
                                      [-1.4171e+09, 1.4909e+06],
                                      [-1.3594e+09, 1.4064e+06],
                                      [-1.1304e+09, 3.0758e+06],
                                      [-1.2212e+09, 2.3546e+06],
                                      [-1.2559e+09, 2.5200e+06],
                                      [-1.2328e+09, 2.1414e+06],
                                      [-1.1660e+09, 2.4119e+06],
                                      [-1.3471e+09, 2.0497e+06],
                                      [-1.1014e+09, 2.6169e+06],
                                      [-1.4985e+09, 1.8238e+06],
                                      [-1.3632e+09, 1.9261e+06],
                                      [-1.2256e+09, 1.9693e+06],
                                      [-1.3750e+09, 1.8936e+06],
                                      [-1.3500e+09, 1.8113e+06],
                                      [-1.1873e+09, 2.3595e+06],
                                      [-1.0606e+09, 2.9955e+06],
                                      [-1.0846e+09, 2.9955e+06],
                                      [-1.3413e+09, 1.8183e+06],
                                      [-1.2558e+09, 2.0020e+06],
                                      [-1.1230e+09, 2.4175e+06],
                                      [-9.2143e+08, 2.9955e+06],
                                      [-9.4374e+08, 2.7797e+06],
                                      [-1.1152e+09, 2.1396e+06],
                                      [-9.3122e+08, 2.8201e+06],
                                      [-1.2600e+09, 1.8436e+06],
                                      [-1.1335e+09, 2.0367e+06],
                                      [-1.3667e+09, 1.7390e+06]], dtype=torch.float64)

train_obj_pr = torch.tensor([[-1.2150e+09, 2.5337e+06],
                             [-1.2379e+09, 1.4909e+06],
                             [-1.2298e+09, 2.2004e+06],
                             [-1.1993e+09, 1.9187e+06],
                             [-1.3148e+09, 1.9170e+06],
                             [-1.3547e+09, 1.7259e+06],
                             [-1.2916e+09, 1.4909e+06],
                             [-1.1373e+09, 2.4322e+06],
                             [-1.1707e+09, 2.9404e+06],
                             [-1.2280e+09, 1.4909e+06],
                             [-1.2844e+09, 1.4070e+06],
                             [-1.2310e+09, 2.2032e+06],
                             [-1.2564e+09, 2.3025e+06],
                             [-1.2311e+09, 1.4151e+06],
                             [-1.3042e+09, 1.4909e+06],
                             [-1.3076e+09, 1.9130e+06],
                             [-1.3947e+09, 1.3862e+06],
                             [-1.4033e+09, 1.4028e+06],
                             [-1.4767e+09, 1.4909e+06],
                             [-1.3228e+09, 1.4078e+06],
                             [-1.1717e+09, 2.9378e+06],
                             [-1.1747e+09, 2.9386e+06],
                             [-1.1802e+09, 2.9396e+06],
                             [-1.1762e+09, 2.9399e+06],
                             [-1.1755e+09, 2.9383e+06],
                             [-1.2009e+09, 2.9222e+06],
                             [-1.1697e+09, 2.9449e+06],
                             [-1.1421e+09, 2.4509e+06],
                             [-1.2119e+09, 2.1743e+06],
                             [-1.1288e+09, 2.4957e+06],
                             [-1.1177e+09, 2.5626e+06],
                             [-1.1806e+09, 2.9350e+06],
                             [-1.1473e+09, 2.9509e+06],
                             [-1.1437e+09, 2.9313e+06],
                             [-1.1379e+09, 2.9652e+06],
                             [-1.1358e+09, 2.9556e+06],
                             [-1.1307e+09, 2.9471e+06],
                             [-1.1219e+09, 2.9468e+06],
                             [-1.1318e+09, 2.9517e+06],
                             [-1.1319e+09, 2.9284e+06],
                             [-1.1139e+09, 2.9457e+06],
                             [-1.1286e+09, 2.9037e+06],
                             [-1.1451e+09, 2.7300e+06],
                             [-1.1189e+09, 2.9647e+06],
                             [-1.0873e+09, 2.8387e+06],
                             [-1.0722e+09, 2.9063e+06],
                             [-1.1220e+09, 2.8867e+06],
                             [-1.0166e+09, 2.8886e+06],
                             [-1.0522e+09, 2.6900e+06],
                             [-1.0436e+09, 2.9767e+06],
                             [-1.0435e+09, 2.9955e+06],
                             [-1.0153e+09, 2.8904e+06],
                             [-1.0535e+09, 2.9955e+06],
                             [-1.0539e+09, 2.9955e+06],
                             [-1.1182e+09, 2.9369e+06],
                             [-1.0068e+09, 2.9901e+06],
                             [-1.0591e+09, 2.9955e+06],
                             [-1.0471e+09, 2.9955e+06],
                             [-1.0658e+09, 2.9955e+06],
                             [-1.0716e+09, 2.9955e+06],
                             [-1.0596e+09, 2.9955e+06],
                             [-1.0796e+09, 2.9955e+06],
                             [-1.0722e+09, 2.9638e+06],
                             [-1.0724e+09, 2.9955e+06],
                             [-1.0542e+09, 2.9955e+06],
                             [-9.9825e+08, 2.9814e+06],
                             [-1.0662e+09, 2.9955e+06],
                             [-1.0749e+09, 2.9955e+06],
                             [-1.0464e+09, 2.9502e+06],
                             [-1.0599e+09, 2.9955e+06],
                             [-1.1443e+09, 2.9801e+06],
                             [-1.0675e+09, 2.9955e+06],
                             [-1.0704e+09, 2.9955e+06],
                             [-1.0834e+09, 2.9955e+06]], dtype=torch.float64)
if __name__ == "__main__":
    SMOKE_test = False
    ref_rate = 10
    [hv_pr_mean, hv_apr_mean, hv_cont_mean] = hp_calculation(y_cont=train_obj_pr, y_apr=train_obj_pr_analytic,
                                                             y_pr=train_obj_pr, ref_rate=ref_rate)
    print(hv_cont_mean[-1])
    print(hv_pr_mean[-1])
    print(hv_apr_mean[-1])
